# 内容拆解复习

## 一、HDFS读写流程：

### 1.1、写流程：

```
	首先客户端向namenode申请写入，namenode会去检查元数据中是否已经存在该文件，若是该文件已经存在，则向客户端返回文件已存在。若是该文件在元数据中没有记录，则会查看当前哪些datanode有空闲资源可以被写入，之后将这些空闲的datanode节点返回给客户端。客户端会去与这些datanode节点进行通信，建立一条传输管道。最后客户端会把文件切分成一个个的数据块依次传输到这条管道中，直至文件全部写入完成。
```

### 1.2、读流程：

```
	客户端会向namenode申请读文件，namenode回去检查元数据中是否有这个文件，若是没有这个文件则向客户端返回该文件不存在的信息。若是该文件存在，则查看这个文件的数据存储在哪些datanode节点上，然后将这些datanode节点返回给客户端。之后客户端会选择一个最近的datanode节点去读取一个个的数据块，最后在本地拼接成一个完整的文件返回给用户。
```

## 二、副本机制：

### 2.1、副本放置策略

```
	第一个副本会优先放置在客户端所在节点，也就是本地节点上，这样可以加快写入速度，并且减少网络IO。
	第二个副本会被放置在另一个机架的datanode节点上，避免整个机架断电导致数据不可用。
	第三个副本会被放置在第二个副本所在机架上的另一个datanode上，这样只需要跨机架一次，速度更快。
```

### 2.2、节点宕机与副本恢复

```
场景：某个节点宕机，导致副本数不满足3个，接下来会发生什么	
	hdfs的每个datanode会定期向namenode发送心跳，并且携带存储信息，告知namenode它持有哪些数据块。若是在预设时间内namenode没有收到某个datanode节点的心跳，就会认为该节点宕机了，然后namenode就会在元数据中把这个datanode标记为不可用。然后namenode就会扫描元数据，看看哪些数据块的副本数少于预设的数量。最后namenode会选择一个健康副本所在的datanode作为源，并且选择另一个健康的datanode作为目标，组成一个数据管道，然后完成数据的复制，最后在更新这个数据块的元数据。
```

### 2.3、如果在写入过程中发生了宕机，namenode是如何恢复写入的。

```
	客户端向namenode发送写请求时，namenode中的元数据如果没有这个文件并且用户也有写权限，那么namenode会先将这个写操作写入到编辑日志中，然后再更新内存中的元数据，最后才会向客户端返回满足条件的datanode，然后让客户端与datanode进行通信进行数据写入。
```

## 三、数据存储格式

### 3.1、OCR

```markdown
高度优化的列式存储格式
（1）优点：
	① 极高的压缩比：
		采用字典编码、行程长度编码、位打包等多种编码技术
		同一列数据相似度高，压缩效果极好，通常比TextFile节省10倍以上空间
	② 极快的查询性能：
		列裁剪：只读取需要的列
		强大的谓词下推：内置索引（默认每10000行一个）、布隆过滤器和列统计信息（min,max,sum,count），可以在读取时快速跳过不满足条件的数据块。
		向量化执行：支持hive的向量化查询，一次处理一批数据，大幅减少CPU开销。
	③ 对hive支持最完善：
		原生支持ACID事务：是hive实现事务、更新、删除操作的基础、
		完美支持hive的所有复杂数据类型（STRUCT、MAP、LIST）
（2）缺点：
	① 生态系统支持却逊于Parquet：虽然在spark、presto等中都支持，但Parquet支持范围更广。
	② 非人类可读
（3）适用场景：hive数据仓库的核心表、ETL过程中的中间表、需要进行大量聚合查询和全表扫描的分析任务。
```

### 3.2、Parquet

```markdown
面向列的存储格式，旨在成为跨生态系统的通用标准。
（1）优点：
	① 极高的压缩比和查询性能：与ORC类似，具有优秀的压缩效率和列裁剪、谓词下推能力
	② 最广泛的生态系统支持：被spark、impala、presto、hive、flink等几乎所有主流大数据引擎原生首选支持，是数据湖的事实标准。
	③ 支持嵌套数据结构：可以非常高效的存储和解析复杂的嵌套数据（如多层JSON），优于ORC。
	④ 支持新增列、修改列等模式变更
（2）缺点：
	① 对hive ACID的支持不如ORC完善
	② 非人类可读
（3）适用场景：数据湖、跨平台、多计算引擎的共享数据存储、spark处理流程的默认格式、包含大量嵌套结构的数据。
```

## 四、数据倾斜处理方案

### 4.1、数据倾斜识别

```sql
-- 通过对key进行分组，分别求每个key的count值，查找出数据量非常大的key
--  1. 识别热点key
SELECT key, COUNT(*) as cnt 
FROM your_table 
GROUP BY key 
ORDER BY cnt DESC 
LIMIT 10;

-- 2. 查看数据分布
SELECT 
    CASE WHEN cnt < 100 THEN '小量级'
         WHEN cnt < 10000 THEN '中量级' 
         ELSE '大量级' END as level,
    COUNT(*) as key_count,
    SUM(cnt) as total_records
FROM (
    SELECT key, COUNT(*) as cnt
    FROM your_table 
    GROUP BY key
) t
GROUP BY 
    CASE WHEN cnt < 100 THEN '小量级'
         WHEN cnt < 10000 THEN '中量级' 
         ELSE '大量级' END;
```

### 4.2、处理方案

#### 4.2.1、过滤无效数据（适用场景：倾斜由NULL、默认值等无效数据引起）

```sql
-- 使用过滤条件将NULL值过滤掉，如果NULL值有意义，可以使用UNION ALL将没有NULL值的数据与有NULL值的数据分别处理并拼接起来

-- 示例：处理NULL值导致的倾斜
SELECT *
FROM orders o
JOIN customers c ON o.customer_id = c.customer_id
WHERE o.customer_id IS NOT NULL  -- 过滤NULL值
  AND c.customer_id IS NOT NULL;

-- 如果NULL值有业务意义，可以单独处理
SELECT *
FROM orders o
JOIN customers c ON o.customer_id = c.customer_id
WHERE o.customer_id IS NOT NULL AND c.customer_id IS NOT NULL

UNION ALL

-- 单独处理NULL值
SELECT o.*, c.* 
FROM orders o
LEFT JOIN customers c ON o.customer_id = c.customer_id
WHERE o.customer_id IS NULL;
```

#### 4.2.2、使用mapjoin（小表join大表）

```sql
-- 自动MapJoin（Hive自动判断）
SET hive.auto.convert.join = true;
SET hive.auto.convert.join.noconditionaltask.size = 512000000; -- 512MB

-- 手动指定MapJoin
SELECT /*+ MAPJOIN(small_table) */ *
FROM big_table 
JOIN small_table ON big_table.key = small_table.key;

-- 多个小表MapJoin
SELECT /*+ MAPJOIN(d1, d2) */ *
FROM fact_table f
JOIN dim_table1 d1 ON f.key1 = d1.key
JOIN dim_table2 d2 ON f.key2 = d2.key;
```

#### 4.2.3、随机数打散（适用场景：大表join大表，存在严重数据倾斜）

##### 4.2.3.1、识别热点key

```sql
-- 找出热点key（比如customer_id=0代表匿名用户）
SELECT customer_id, COUNT(*) as cnt
FROM orders 
GROUP BY customer_id 
ORDER BY cnt DESC 
LIMIT 5;
```

##### 4.2.3.2、对热点key进行加盐

```sql
-- 创建临时视图，对热点key加随机后缀
CREATE VIEW salted_orders AS
SELECT 
    order_id,
    order_amount,
    CASE 
        WHEN customer_id = 0  -- 热点key
        THEN CONCAT(customer_id, '_', CAST(floor(rand() * 10) as string))
        ELSE CAST(customer_id as string)
    END as salted_customer_id
FROM orders;
```

##### 4.2.3.3、使用加盐后的key进行join

```sql
SELECT 
    o.order_id,
    o.order_amount, 
    c.customer_name
FROM salted_orders o
JOIN expanded_customers c ON o.salted_customer_id = c.salted_customer_id;
```

#### 4.2.4、skew join参数优化（适用场景：轻度数据倾斜，自动处理）

```sql
-- 开启Skew Join优化
SET hive.optimize.skewjoin = true;
SET hive.skewjoin.key = 100000; -- 超过10万行认为是倾斜key

-- 或者使用更智能的配置
SET hive.skewjoin.mapjoin.map.tasks = 10000;
SET hive.skewjoin.mapjoin.min.split = 33554432;

-- 执行查询
SELECT *
FROM big_table1 t1
JOIN big_table2 t2 ON t1.key = t2.key;
```

#### 4.2.5、两阶段聚合

```sql
-- 原始查询（可能倾斜）
SELECT user_id, COUNT(*) as cnt
FROM user_actions 
GROUP BY user_id;

-- 改进：两级聚合
SELECT user_id, SUM(partial_cnt) as total_cnt
FROM (
    -- 第一级：局部聚合（可以加随机数打散）
    SELECT 
        user_id,
        CAST(rand() * 10 as int) as bucket,  -- 0-9的随机桶
        COUNT(*) as partial_cnt
    FROM user_actions 
    GROUP BY user_id, CAST(rand() * 10 as int)
) first_stage
GROUP BY user_id;  -- 第二级：全局聚合
```

### 4.3、预防数据倾斜最佳实践

```
1、数据质量监控：定期检查数据分布，识别潜在的热点key
2、表设计优化：合理使用分区和分桶，避免天然的数据倾斜
3、ETL过程优化：在数据接入阶段就对可能的热点key进行处理
4、参数调优：根据集群规模和数据特性调整相关参数
5、监控告警：对长时间运行的作业建立监控和告警机制
```

